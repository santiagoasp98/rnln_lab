{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9ZNGOygrFsT"
      },
      "source": [
        "Redes Neuronales para Lenguaje Natural, 2025\n",
        "\n",
        "---\n",
        "# Laboratorio 2\n",
        "\n",
        "En este laboratorio construiremos un sistema de Question Answering (QA) utilizando el método de Retrieval-Augmented Generation (RAG), que implica el uso de un paso de recuperación de información y un paso de generación de respuesta con LLM.\n",
        "\n",
        "**Entrega: 18/11**\n",
        "\n",
        "**Se debe entregar un archivo zip que contenga:**\n",
        "* Este notebook de Python (.ipynb) completo.\n",
        "* Los documentos obtenidos y utilizados como fuentes de información según se explica en la parte 1 (opcionalmente se puede entregar un archivo CSV con los textos de cada documento).\n",
        "* Archivo CSV con el conjunto de preguntas y respuestas como se explica en la parte 5.\n",
        "\n",
        "**No olvidar mantener todas las salidas de cada región de código en el notebook!**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lJHOJf9OVMci"
      },
      "outputs": [],
      "source": [
        "#@title Instalar librerias\n",
        "!pip install transformers\n",
        "!pip install bitsandbytes\n",
        "!pip install accelerate\n",
        "!pip install sentence-transformers\n",
        "!pip install evaluate\n",
        "!pip install bert_score\n",
        "!pip install wikipedia-api\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "hKuaFtkc5Esq"
      },
      "outputs": [],
      "source": [
        "#@title Estilo de salida de colab\n",
        "from IPython.display import HTML, display\n",
        "pre_run_cell_fn = lambda: display(HTML('''<style> pre {white-space: pre-wrap;}</style>'''))\n",
        "get_ipython().events.register('pre_run_cell', pre_run_cell_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtgdalArNvHn"
      },
      "source": [
        "## Parte 1: Procesamiento de los documentos\n",
        "\n",
        "En esta parte, cada grupo deberá construir y procesar su conjunto de documentos. Esto consiste de los siguientes pasos:\n",
        "\n",
        "* Elegir un tema dentro de un dominio específico sobre el que trabajar.\n",
        "* Obtener al menos 5 documentos en español que contengan información sobre el tema elegido.\n",
        "* Procesar cada documento para extraer el texto del formato original a un string en Python (por ejemplo, extraer el texto de un PDF).\n",
        "\n",
        "El resultado de esta parte debe ser una lista cargada en memoria que contenga el texto (string) de cada uno de los documentos elegidos.\n",
        "\n",
        "**Sugerencias:**\n",
        "* Se recomienda utilizar artículos de wikipedia para simplificar la etapa de extracción del texto (ver la librería [wikipedia-api](https://github.com/martin-majlis/Wikipedia-API/)).\n",
        "* Opcionalmente puede utilizar documentos PDF, páginas web u otros formatos. En estos casos se sugiere:\n",
        "  * Utilizar la librería PyPDF2 para procesar documentos PDF.\n",
        "  * Utilizar la librería LangChain para procesar páginas web, en particular la clase Html2TextTransformer, que convierte HTML a Markdown ([ejemplo de uso](https://python.langchain.com/v0.2/docs/integrations/document_transformers/html2text/)).\n",
        "* Puede ser conveniente guardar el resultado del procesamiento de los documentos en un archivo CSV (donde cada fila corresponde al texto de un documento) para no tener que repetir este proceso cada vez que se ejecuta el notebook, y en su lugar cargar el archivo CSV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install pymupdf4llm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Usé PyMuPDF en vez de PyPDF2 para poder pasar el pdf a markdown.\n",
        "\n",
        "Duda -> No sé si usar la página web o el pdf, la página web no tiene todo el documento cargado a priori, hay que scrollear en el iFrame para que aparezca en el html, no sé si suma tanta data tampoco"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pymupdf4llm\n",
        "import csv\n",
        "\n",
        "CSV_NAME = \"corpus.csv\"\n",
        "\n",
        "def extractTextFromPdf(file_name):\n",
        "    markdown = pymupdf4llm.to_markdown(file_name)\n",
        "    return markdown\n",
        "\n",
        "def replaceCitations(text):\n",
        "    return text\n",
        "\n",
        "def writeIntoCsv(file_name, text):\n",
        "    with open(CSV_NAME, \"w\", newline=\"\") as csv_file:\n",
        "        writer = csv.writer(csv_file)\n",
        "        writer.writerow([file_name, text])\n",
        "\n",
        "def addFileToCsv(file_name):\n",
        "    text = extractTextFromPdf(file_name)\n",
        "    text = replaceCitations(text)\n",
        "    writeIntoCsv(file_name, text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XuotA7OMHahn"
      },
      "outputs": [],
      "source": [
        "files = [\"test.pdf\"]\n",
        "for file_name in files:\n",
        "  addFileToCsv(file_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qtOV9zSSRul"
      },
      "source": [
        "Los textos resultantes deben estar almacenados en la variable `documents`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGkUEe10SQrT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "documents = pd.read_csv(CSV_NAME, )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCpCYmSrN6oK"
      },
      "source": [
        "*italicized text*## Parte 2: Chunking\n",
        "\n",
        "Una vez que se obtiene el texto de cada documento, se debe realizar la etapa de _chunking_. Esta etapa consiste en dividir cada texto en segmentos más chicos a los que llamamos _chunks_.\n",
        "\n",
        "Realizar la etapa de _chunking_ de forma automática utilizando un método simple que permita obtener _chunks_ de un largo aproximado de 500 caracteres.\n",
        "\n",
        "Puede probar con dividir a nivel de caracteres, palabras o incluso párrafos, teniendo en cuenta que el largo de cada _chunk_ no debería exceder demasiado los 500 caracteres.\n",
        "\n",
        "**Sugerencias:**\n",
        "* Puede utilizar los splitters disponibles en LangChain ([documentación](https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/)) como RecursiveCharacterTextSplitter, aunque no es obligatorio y también es correcto hacer una implementación propia.\n",
        "* Tener en cuenta que esta etapa es crucial en el resultado final. Cuanto más contextualizados queden los *chunks*, mejor será el rendimiento de la etapa de recuperación de información. Es conveniente minimizar la división de palabras (o párrafos) por la mitad."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4npdH7vjiV1x"
      },
      "source": [
        "### Documento auxiliar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LE5ZkBLRJQH",
        "outputId": "f2344b4f-2844-424c-e252-a7fe84451959"
      },
      "outputs": [],
      "source": [
        "# DOCUMENTO AUXILIAR PARA PRUEBAS\n",
        "file_path = 'birds.md'\n",
        "\n",
        "try:\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        birds_document = f.read()\n",
        "    print(f\"--- Successfully loaded content from '{file_path}' ---\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at '{file_path}'\")\n",
        "    print(\"Please double-check the filename. It must be an exact match.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-LtdDo5ibnW"
      },
      "source": [
        "### Codigo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61AgTDuPXjMu",
        "outputId": "e0d3d0ad-bb46-4b41-f9c3-c680f029d475"
      },
      "outputs": [],
      "source": [
        "!pip install langchain -q\n",
        "print(\"--- LangChain Installed ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQ9VJF4oitZU"
      },
      "source": [
        "#### Definición de splitters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMDHuSYjiTG3"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
        "\n",
        "from enum import StrEnum\n",
        "\n",
        "class SplitterType(StrEnum):\n",
        "    RECURSIVE = \"Recursive\"\n",
        "    MARKDOWN = \"Markdown\"\n",
        "\n",
        "## Recursive Splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 450,\n",
        "    chunk_overlap  = 40,\n",
        "    length_function = len,\n",
        "    is_separator_regex = False,\n",
        ")\n",
        "\n",
        "def splitRecursivo(text):\n",
        "  chunks = text_splitter.split_text(text)\n",
        "  return chunks\n",
        "\n",
        "## Markdown Splitter\n",
        "headers_to_split_on = [\n",
        "    (\"#\", \"Header 1\"),\n",
        "    (\"##\", \"Header 2\"),\n",
        "    (\"###\", \"Header 3\"),\n",
        "    (\"###\", \"Header 4\"),\n",
        "]\n",
        "\n",
        "markdown_splitter = MarkdownHeaderTextSplitter(\n",
        "    headers_to_split_on,\n",
        "    return_each_line=True\n",
        ")\n",
        "\n",
        "def splitMarkdown(text):\n",
        "  markdown_chunks = markdown_splitter.split_text(text)\n",
        "  chunks = text_splitter.split_documents(markdown_chunks)\n",
        "  return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PgtMbHjVLEj"
      },
      "outputs": [],
      "source": [
        "def chunk_text(text,splitter):\n",
        "\n",
        "  if splitter == SplitterType.RECURSIVE:\n",
        "    chunks = splitRecursivo(text)\n",
        "  elif splitter == SplitterType.MARKDOWN:\n",
        "    chunks = splitMarkdown(text)\n",
        "\n",
        "  return chunks # Lista de strings con los chunks del texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dM7jlMxFotEm"
      },
      "source": [
        "### Codigos para prueba y observar resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCvEpguOjrGs",
        "outputId": "d49f11bd-5af3-49e6-aad3-8692c4acf88b"
      },
      "outputs": [],
      "source": [
        "## Recursivo\n",
        "\n",
        "chunks = chunk_text(birds_document,SplitterType.RECURSIVE)\n",
        "\n",
        "# --- 6. View the Results ---\n",
        "print(f\"\\n--- Original Document Length: {len(birds_document)} characters ---\")\n",
        "print(f\"--- Total Chunks Generated: {len(chunks)} ---\")\n",
        "\n",
        "print(\"\\n--- Here are the chunks: ---\")\n",
        "for i, chunk in enumerate(chunks[:4]):\n",
        "    print(f\"--- CHUNK {i+1} (Length: {len(chunk)}) ---\")\n",
        "    print(chunk)\n",
        "    print(\"-\" * 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHGgvnyRY97K",
        "outputId": "6a75864d-55ad-4790-d39a-817ca34bce29"
      },
      "outputs": [],
      "source": [
        "## Markdown\n",
        "\n",
        "chunks = chunk_text(birds_document,SplitterType.MARKDOWN)\n",
        "\n",
        "# --- 6. View the Results ---\n",
        "print(f\"\\n--- Original Document Length: {len(birds_document)} characters ---\")\n",
        "print(f\"--- Total Chunks Generated: {len(chunks)} ---\")\n",
        "\n",
        "print(\"\\n--- Here are the chunks: ---\")\n",
        "for i, chunk in enumerate(chunks[:4]):\n",
        "    print(f\"--- CHUNK {i+1} (Length: {len(chunk.page_content)}) ---\")\n",
        "    print(chunk)\n",
        "    print(\"-\" * 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUVcX8t5udny"
      },
      "outputs": [],
      "source": [
        "## Si se quiere incorporar los títulos al texto\n",
        "\n",
        "def document_to_list(chunks):\n",
        "  chunks_list = []\n",
        "\n",
        "  for chunk in chunks:\n",
        "      # --- 1. Get and sort headers ---\n",
        "      header_keys = [k for k in chunk.metadata.keys() if k.startswith('Header ')]\n",
        "\n",
        "      # Sort the keys numerically, not alphabetically\n",
        "      # 'key=lambda k: int(k.split(' ')[1])' turns 'Header 10' into 10\n",
        "      header_keys.sort(key=lambda k: int(k.split(' ')[1]))\n",
        "\n",
        "      # --- 2. Build the header string ---\n",
        "\n",
        "      # Get the actual header text for each key\n",
        "      header_values = [chunk.metadata[k] for k in header_keys]\n",
        "\n",
        "      # Join them with newlines\n",
        "      header_string = \"\\n\".join(header_values)\n",
        "\n",
        "      # --- 3. Concatenate and append ---\n",
        "\n",
        "      # Add a newline between the headers and the page content\n",
        "      final_string = f\"{header_string}\\n{chunk.page_content}\"\n",
        "\n",
        "      chunks_list.append(final_string)\n",
        "\n",
        "  return chunks_list\n",
        "\n",
        "chunks_list = document_to_list(chunks)\n",
        "\n",
        "print(f\"--- Generated {len(chunks_list)} strings in chunks_list ---\")\n",
        "\n",
        "print(\"\\n--- Example: First string ---\")\n",
        "print(chunks_list[0])\n",
        "\n",
        "print(\"\\n--- Example: Fourth string (with two headers) ---\")\n",
        "print(chunks_list[3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34APo88iuqoL",
        "outputId": "c21407d2-f36c-4054-afce-34d144093a19"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jz42mD-ytmby"
      },
      "source": [
        "#### Métricas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jW2ZErc9fFur",
        "outputId": "6a9bc039-a38d-4c49-ceb4-4ff772d7fd8a"
      },
      "outputs": [],
      "source": [
        "all_lengths = [len(chunk.page_content) for chunk in chunks]\n",
        "\n",
        "ordered_lengths = sorted(all_lengths)\n",
        "\n",
        "average_length = sum(all_lengths) / len(all_lengths)\n",
        "\n",
        "print(f\"Average chunk length: {average_length:.2f} characters\")\n",
        "print(f\"Median chunk length: {ordered_lengths[42]} characters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7h_kw07sZwp"
      },
      "source": [
        "## Experimentando alternativas\n",
        "\n",
        "Esta parte tiene ideas que podrian ser utiles para la recuperación. Por ahora es solo copy-paste de código portencialmente útil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtSklh9wf4br",
        "outputId": "8c8aefcf-2d66-447e-928a-d5dda748ca81"
      },
      "outputs": [],
      "source": [
        "import pprint\n",
        "\n",
        "def build_header_index(chunks):\n",
        "    \"\"\"Builds a nested dictionary index from chunk metadata.\"\"\"\n",
        "    header_index = {}\n",
        "\n",
        "    for chunk in chunks:\n",
        "        # Get all header keys and sort them numerically\n",
        "        # (e.g., 'Header 1', 'Header 2', 'Header 10')\n",
        "        header_keys = sorted(\n",
        "            [k for k in chunk.metadata.keys() if k.startswith('Header ')],\n",
        "            key=lambda k: int(k.split(' ')[1])\n",
        "        )\n",
        "\n",
        "        # Get the actual header text values\n",
        "        header_path = [chunk.metadata[k] for k in header_keys]\n",
        "\n",
        "        # --- Build the nested dictionary ---\n",
        "        current_level = header_index\n",
        "        for header in header_path:\n",
        "            if header not in current_level:\n",
        "                current_level[header] = {}  # Create a new branch\n",
        "            current_level = current_level[header] # Move down the tree\n",
        "\n",
        "    return header_index\n",
        "\n",
        "# --- Run the function and print the result ---\n",
        "index = build_header_index(chunks)\n",
        "\n",
        "print(\"--- Nested Header Index ---\")\n",
        "pprint.pprint(index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsmvsojztGDZ"
      },
      "outputs": [],
      "source": [
        "def get_chunks_by_path(chunks, header_path):\n",
        "    \"\"\"\n",
        "    Finds all chunks that match a specific header path.\n",
        "\n",
        "    A chunk matches if its metadata path starts with the provided header_path.\n",
        "    \"\"\"\n",
        "    matching_chunks = []\n",
        "    len_path = len(header_path)\n",
        "\n",
        "    for chunk in chunks:\n",
        "        # Get all header keys and sort them numerically\n",
        "        header_keys = sorted(\n",
        "            [k for k in chunk.metadata.keys() if k.startswith('Header ')],\n",
        "            key=lambda k: int(k.split(' ')[1])\n",
        "        )\n",
        "\n",
        "        # Get the chunk's full header path\n",
        "        chunk_path = [chunk.metadata[k] for k in header_keys]\n",
        "\n",
        "        # Check if the chunk's path starts with the user's path\n",
        "        if chunk_path[:len_path] == header_path:\n",
        "            matching_chunks.append(chunk)\n",
        "\n",
        "    return matching_chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Nyqa9i7tKSq",
        "outputId": "f4b34e9a-b265-4265-ef5f-ae3ac30656d0"
      },
      "outputs": [],
      "source": [
        "path_h2 = [\n",
        "    '**A Comprehensive Look at the World of Birds**',\n",
        "    '**The Avian Lineage: From Dinosaurs to Modern Birds**'\n",
        "]\n",
        "\n",
        "results_h2 = get_chunks_by_path(chunks, path_h2)\n",
        "\n",
        "print(f\"\\n--- Found {len(results_h2)} chunks for path: {path_h2} ---\")\n",
        "for i, chunk in enumerate(results_h2):\n",
        "    print(f\"Chunk {i+1} Content: {chunk.page_content}\")\n",
        "    print(f\"Chunk {i+1} Metadata: {chunk.metadata}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74-bH4j_VrRT"
      },
      "outputs": [],
      "source": [
        "chunks = []\n",
        "for document in documents:\n",
        "  chunks += chunk_text(document)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJ2wgEx3N-O3"
      },
      "source": [
        "## Parte 3: Recuperación de información\n",
        "\n",
        "En esta parte vamos a implementar el método de recuperación de información que nos permitirá obtener los _chunks_ más relevantes para la pregunta.\n",
        "\n",
        "En primer lugar, cargamos el modelo Bi-Encoder que utilizaremos para generar los embeddings utilizando la librería sentence_transformers.\n",
        "\n",
        "Se utiliza el modelo multilingüe [intfloat/multilingual-e5-large](https://huggingface.co/intfloat/multilingual-e5-large), fine-tuning del modelo `xlm-roberta-large` para la tarea de generación de sentence embeddings.\n",
        "\n",
        "Se pueden explorar otros modelos Bi-Encoder, e incluso modelos Cross-Encoder o del tipo ColBERT. En HuggingFace se puede consultar el siguiente [leaderboard](https://huggingface.co/spaces/mteb/leaderboard) que compara varios modelos de este tipo en diferentes tareas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Fozn2H7vZsj"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model_emb = SentenceTransformer(\"intfloat/multilingual-e5-large\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwuNrG2djc3f"
      },
      "source": [
        "A continuación se debe generar las representaciones vectoriales para todos los _chunks_ ([ejemplo de uso](https://huggingface.co/intfloat/multilingual-e5-large#support-for-sentence-transformers)).\n",
        "\n",
        "**Observación:** El modelo que estamos usando espera que los _chunks_ comiencen con el prefijo `passage: ` por lo que será necesario agregarlo al inicio de todos los _chunks_."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeBfJ_Ubjb3k"
      },
      "outputs": [],
      "source": [
        "# Su código aquí"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N62kl88KwZ-u"
      },
      "source": [
        "Por último, se debe implementar el algoritmo de búsqueda de los embeddings más cercanos para un embedding dado.\n",
        "\n",
        "**Sugerencias:**\n",
        "* Utilizar la clase NearestNeighbors de sklearn ([documentación](https://scikit-learn.org/dev/modules/generated/sklearn.neighbors.NearestNeighbors.html#sklearn.neighbors.NearestNeighbors))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5bGq2Etxkj-"
      },
      "outputs": [],
      "source": [
        "# Su código aquí"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwMSqjQUOEzQ"
      },
      "source": [
        "## Parte 4: Generación de respuestas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyPn_Cwtq81r"
      },
      "source": [
        "### Configuración de LLM\n",
        "\n",
        "Utilizaremos el modelo **Llama 3.1** de Meta a través de la plataforma [HuggingFace](https://huggingface.co/). Para poder usar este modelo en HuggingFace es necesario seguir los siguientes pasos:\n",
        "\n",
        "- Crearse una cuenta de HuggingFace (https://huggingface.co/)\n",
        "- Aceptar los términos para usar el modelo en HuggingFace, que aparecen en el siguiente enlace: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct\n",
        "- Crear un token de HuggingFace con permiso de lectura siguiendo el siguiente enlace: https://huggingface.co/settings/tokens\n",
        "- Ejecutar la siguiente celda e ingresar el token creado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpJk4QSGAPkE"
      },
      "outputs": [],
      "source": [
        "# Ejecutar para conectarse a HuggingFace\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fPqX6DH1Gzn"
      },
      "source": [
        "A continuación se inicializan el tokenizer y el modelo cuantizado a 4 bits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVPu5MUiTBog"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "# Inicializar el tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "  \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        ")\n",
        "\n",
        "# Configuración de cuantización a 4 bits (para mejorar eficiencia)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "  load_in_4bit=True,\n",
        "  bnb_4bit_quant_type=\"nf4\",\n",
        "  bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Inicializar el modelo\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "  \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
        "  quantization_config=bnb_config,\n",
        "  device_map=\"auto\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgGESsQX1k5_"
      },
      "source": [
        "Creamos ahora dos funciones auxiliares que usaremos para la generación de las respuestas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jyf3eQPB4zQz"
      },
      "outputs": [],
      "source": [
        "# Generar respuesta\n",
        "from transformers import GenerationConfig, pipeline\n",
        "\n",
        "def get_response(prompt, temp=0.0, max_tok=500):\n",
        "  # Configuración de temperatura\n",
        "  generation_config = GenerationConfig(\n",
        "    temperature = temp if temp > 0 else None,\n",
        "    do_sample = temp > 0\n",
        "  )\n",
        "\n",
        "  # Inicializar pipeline para generación de texto\n",
        "  pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    config=generation_config,\n",
        "    tokenizer=tokenizer,\n",
        "    pad_token_id = tokenizer.eos_token_id\n",
        "  )\n",
        "\n",
        "  # Generar texto\n",
        "  output = pipe(\n",
        "      prompt,\n",
        "      return_full_text=False,\n",
        "      max_new_tokens=max_tok\n",
        "    )\n",
        "\n",
        "  return output[0]['generated_text']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PN9wyhSm5Mtj"
      },
      "source": [
        "### Crear prompt y generar respuesta\n",
        "\n",
        "Escribir la función `create_prompt(question)` que dada una pregunta, genere la prompt que se utilizará para generar la respuesta. Tener en cuenta que se debe realizar la búsqueda semántica de los _chunks_ más cercanos a la pregunta utilizando lo implementado en la parte 3.\n",
        "\n",
        "**Observación:** Al igual que para los _chunks_, el modelo Bi-Encoder espera que la pregunta comience con un prefijo especial: `query: ` por lo que será necesario agregarlo al inicio de la pregunta para generar el embedding.\n",
        "\n",
        "**Sugerencias:**\n",
        "* Puede probar con distintas cantidades de _chunks_ recuperados, pero se sugiere comenzar con 3. Tener en cuenta que más _chunks_ recuperados y agregados en la prompt implica mayor uso de memoria en inferencia.\n",
        "* Utilizar la función `apply_chat_template` del tokenizer para aplicar el template correcto del modelo Llama 3.1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQtVcehH-nQJ"
      },
      "outputs": [],
      "source": [
        "def create_prompt(question):\n",
        "  # Su código aquí\n",
        "\n",
        "  return prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfYv8FZ2-0fR"
      },
      "source": [
        "Probar la prompt anterior con un ejemplo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqESfBkq-wF6"
      },
      "outputs": [],
      "source": [
        "question = \"\" # Completar con una pregunta adecuada al contexto\n",
        "prompt = create_prompt(question)\n",
        "print(\"PROMPT:\")\n",
        "print(prompt)\n",
        "\n",
        "print(\"\\nRESPUESTA:\")\n",
        "print(get_response(prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYOvIEHAOJSc"
      },
      "source": [
        "## Parte 5: Evaluación\n",
        "A continuación vamos a evaluar la solución construida. Para ello, se deben seguir los siguientes pasos:\n",
        "\n",
        "* Construir un conjunto de evaluación de forma manual que contenga al menos 12 preguntas y respuestas con las siguientes características:\n",
        "  * Al menos 3 preguntas deben necesitar información presente en más de un _chunk_ para ser respondidas correctamente.\n",
        "  * Al menos 3 preguntas no deben estar relacionadas con el dominio, y su respuesta de referencia debe ser algo similar a: \"Lo siento, no cuento con información para responder esa pregunta.\"\n",
        "* El conjunto debe estar en un archivo CSV llamado testset.csv, con las columnas \"question\" y \"answer\".\n",
        "\n",
        "Se deberá realizar al menos tres experimentos diferentes y evaluar sobre el mismo conjunto de test con la métrica BERTScore. Los experimentos deben variar en al menos uno de los siguientes elementos:\n",
        "* Método de chunking\n",
        "* Modelo (o método) de retrieval\n",
        "* Modelo de generación (LLM)\n",
        "* Método de prompting (se puede probar con few-shot, chain of thought, etc)\n",
        "* Otros aspectos que considere relevantes a probar\n",
        "\n",
        "A continuación se definen funciones auxiliares para la evaluación.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkFBE_iEKkkm"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def generate_predictions(questions):\n",
        "  prompts = [create_prompt(question) for question in questions]\n",
        "  predictions = [get_response(prompt) for prompt in tqdm(prompts)]\n",
        "  return predictions\n",
        "\n",
        "def evaluate_predictions(predictions, references):\n",
        "  bertscore = evaluate.load(\"bertscore\")\n",
        "  results = bertscore.compute(predictions=predictions, references=references, lang='es')\n",
        "\n",
        "  print(f\"BERTScore P: {np.array(results['precision']).mean():.3f}\")\n",
        "  print(f\"BERTScore R: {np.array(results['recall']).mean():.3f}\")\n",
        "  print(f\"BERTScore F1: {np.array(results['f1']).mean():.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-wIITRgKKoE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Leer el conjunto de evaluación\n",
        "df = pd.read_csv(\"testset.csv\")\n",
        "\n",
        "# Obtener preguntas y respuestas\n",
        "questions = df[\"question\"].tolist()\n",
        "references = df[\"answer\"].tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8quL9-CVKEB"
      },
      "source": [
        "Evalúe los experimentos realizados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUnRnoksVOv-"
      },
      "outputs": [],
      "source": [
        "# su código aquí\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1GKFIKIVk42"
      },
      "source": [
        "Reportar los resultados obtenidos en los experimentos realizados completando la siguiente tabla:\n",
        "\n",
        "| Exp | Descripción | P BERTScore | R BERTScore | F BERTScore |\n",
        "|-----|-------------|-------------|-------------|-------------|\n",
        "| 1 | | | | |\n",
        "| 2 | | | | |\n",
        "| 3 | | | | |\n",
        "\n",
        "Responda las siguientes preguntas:\n",
        "\n",
        "1. Explique brevemente las diferencias en los experimentos realizados, ¿Qué aspectos se varió en el pipeline de RAG?\n",
        "\n",
        "2. ¿Son consistentes los resultados obtenidos con lo que esperaba?\n",
        "\n",
        "3. ¿Le parece que la métrica BERTScore está capturando correctamente las diferencias de los distintos experimentos realizados?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peHQat3XWzX2"
      },
      "source": [
        "(sus respuestas aquí)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "FtgdalArNvHn",
        "gCpCYmSrN6oK",
        "XJ2wgEx3N-O3",
        "qwMSqjQUOEzQ",
        "LYOvIEHAOJSc"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
