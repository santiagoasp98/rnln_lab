{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9ZNGOygrFsT"
      },
      "source": [
        "Redes Neuronales para Lenguaje Natural, 2025\n",
        "\n",
        "---\n",
        "# Laboratorio 2\n",
        "\n",
        "En este laboratorio construiremos un sistema de Question Answering (QA) utilizando el método de Retrieval-Augmented Generation (RAG), que implica el uso de un paso de recuperación de información y un paso de generación de respuesta con LLM.\n",
        "\n",
        "**Entrega: 18/11**\n",
        "\n",
        "**Se debe entregar un archivo zip que contenga:**\n",
        "* Este notebook de Python (.ipynb) completo.\n",
        "* Los documentos obtenidos y utilizados como fuentes de información según se explica en la parte 1 (opcionalmente se puede entregar un archivo CSV con los textos de cada documento).\n",
        "* Archivo CSV con el conjunto de preguntas y respuestas como se explica en la parte 5.\n",
        "\n",
        "**No olvidar mantener todas las salidas de cada región de código en el notebook!**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7ye5n-Obdr6"
      },
      "source": [
        "## Parte 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-W41uyYxbdr7"
      },
      "source": [
        "### Instalación bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lJHOJf9OVMci"
      },
      "outputs": [],
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "  print('Running on CoLab')\n",
        "  COLAB = True\n",
        "else:\n",
        "  print('Not running on CoLab')\n",
        "  COLAB = False\n",
        "\n",
        "#@title Estilo de salida de colab\n",
        "from IPython.display import HTML, display, clear_output\n",
        "if COLAB:\n",
        "    pre_run_cell_fn = lambda: display(HTML('''<style> pre {white-space: pre-wrap;}</style>'''))\n",
        "    get_ipython().events.register('pre_run_cell', pre_run_cell_fn)\n",
        "\n",
        "import sys\n",
        "!{sys.executable} -m pip install transformers\n",
        "!{sys.executable} -m pip install bitsandbytes\n",
        "!{sys.executable} -m pip install accelerate\n",
        "!{sys.executable} -m pip install sentence-transformers\n",
        "!{sys.executable} -m pip install evaluate\n",
        "!{sys.executable} -m pip install bert_score\n",
        "!{sys.executable} -m pip install google-genai\n",
        "!{sys.executable} -m pip install pymupdf4llm\n",
        "!{sys.executable} -m pip install langchain-text-splitters\n",
        "!{sys.executable} -m pip install --upgrade huggingface_hub\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0flW0h4-bdsC"
      },
      "source": [
        "### Creación dataset preguntas y respuestas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yY-tmi5bdsE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "TEST_DATASET = \"testset.csv\"\n",
        "\n",
        "if not os.path.exists(TEST_DATASET):\n",
        "    with open(TEST_DATASET, \"w\", newline=\"\") as csv_file:\n",
        "        writer = csv.writer(csv_file)\n",
        "        writer.writerow([\"question\", \"answer\"])\n",
        "\n",
        "        # No relacionadas con el dominio\n",
        "        respuesta_para_pregunta_fuera_de_tema = \"No tengo información sobre esa pregunta.\"\n",
        "        writer.writerow([\"¿Dónde nace el río Uruguay?\", respuesta_para_pregunta_fuera_de_tema])\n",
        "        writer.writerow([\"¿En qué año se firmó el tratado de Tordesillas?\", respuesta_para_pregunta_fuera_de_tema])\n",
        "        writer.writerow([\"¿Cuál es el presidente actual de Chile?\", respuesta_para_pregunta_fuera_de_tema])\n",
        "\n",
        "        # Necesitan información de más de un chunk\n",
        "        writer.writerow([\"¿La Biblioteca Nacional siempre estuvo en su lugar actual?\", \"No, la sede de la Biblioteca Nacional ha cambiado varias veces a lo largo de la historia.\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtgdalArNvHn"
      },
      "source": [
        "## Parte 1: Procesamiento de los documentos\n",
        "\n",
        "En esta parte, cada grupo deberá construir y procesar su conjunto de documentos. Esto consiste de los siguientes pasos:\n",
        "\n",
        "* Elegir un tema dentro de un dominio específico sobre el que trabajar.\n",
        "* Obtener al menos 5 documentos en español que contengan información sobre el tema elegido.\n",
        "* Procesar cada documento para extraer el texto del formato original a un string en Python (por ejemplo, extraer el texto de un PDF).\n",
        "\n",
        "El resultado de esta parte debe ser una lista cargada en memoria que contenga el texto (string) de cada uno de los documentos elegidos.\n",
        "\n",
        "**Sugerencias:**\n",
        "* Se recomienda utilizar artículos de wikipedia para simplificar la etapa de extracción del texto (ver la librería [wikipedia-api](https://github.com/martin-majlis/Wikipedia-API/)).\n",
        "* Opcionalmente puede utilizar documentos PDF, páginas web u otros formatos. En estos casos se sugiere:\n",
        "  * Utilizar la librería PyPDF2 para procesar documentos PDF.\n",
        "  * Utilizar la librería LangChain para procesar páginas web, en particular la clase Html2TextTransformer, que convierte HTML a Markdown ([ejemplo de uso](https://python.langchain.com/v0.2/docs/integrations/document_transformers/html2text/)).\n",
        "* Puede ser conveniente guardar el resultado del procesamiento de los documentos en un archivo CSV (donde cada fila corresponde al texto de un documento) para no tener que repetir este proceso cada vez que se ejecuta el notebook, y en su lugar cargar el archivo CSV."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4IWsvVcbdsJ"
      },
      "source": [
        "Usé PyMuPDF en vez de PyPDF2 para poder pasar el pdf a markdown.\n",
        "\n",
        "Duda -> No sé si usar la página web o el pdf, la página web no tiene todo el documento cargado a priori, hay que scrollear en el iFrame para que aparezca en el html, no sé si suma tanta data tampoco\n",
        "\n",
        "TO DO -> Revisar como manejar las citas / notas al pie. Enriquecimiento de texto etc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QJyOeThbdsL",
        "outputId": "3421e122-5e72-44b8-a8b4-5d5992bc083b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Consider using the pymupdf_layout package for a greatly improved page layout analysis.\n"
          ]
        }
      ],
      "source": [
        "import pymupdf4llm\n",
        "import csv\n",
        "\n",
        "CSV_NAME = \"corpus.csv\"\n",
        "\n",
        "def extractTextFromPdf(file_name):\n",
        "    markdown = pymupdf4llm.to_markdown(file_name)\n",
        "    return markdown\n",
        "\n",
        "def writeIntoCsv(file_name, text):\n",
        "    with open(CSV_NAME, \"a\", newline=\"\") as csv_file:\n",
        "        writer = csv.writer(csv_file)\n",
        "        writer.writerow([file_name, text])\n",
        "\n",
        "def addFileToCsv(file_name):\n",
        "    text = extractTextFromPdf(file_name)\n",
        "    writeIntoCsv(file_name, text)\n",
        "\n",
        "def createCsv():\n",
        "    with open(CSV_NAME, \"w\", newline=\"\") as csv_file:\n",
        "        writer = csv.writer(csv_file)\n",
        "        writer.writerow([\"file_name\", \"text\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XuotA7OMHahn"
      },
      "outputs": [],
      "source": [
        "files = [\"Presentacion.pdf\", \"Acuña de Figueroa.pdf\", \"Clausura, expolios, intentos de reapertura.pdf\", \"El edificio de la Biblioteca Nacional.pdf\", \"La Nueva Biblioteca Nacional.pdf\"]\n",
        "\n",
        "createCsv()\n",
        "for file_name in files:\n",
        "  addFileToCsv(file_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qtOV9zSSRul"
      },
      "source": [
        "Los textos resultantes deben estar almacenados en la variable `documents`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGkUEe10SQrT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "documents = pd.read_csv(CSV_NAME, header = 0, index_col='file_name')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1do8RRVbdsX"
      },
      "outputs": [],
      "source": [
        "print(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCpCYmSrN6oK"
      },
      "source": [
        "## Parte 2: Chunking\n",
        "\n",
        "Una vez que se obtiene el texto de cada documento, se debe realizar la etapa de _chunking_. Esta etapa consiste en dividir cada texto en segmentos más chicos a los que llamamos _chunks_.\n",
        "\n",
        "Realizar la etapa de _chunking_ de forma automática utilizando un método simple que permita obtener _chunks_ de un largo aproximado de 500 caracteres.\n",
        "\n",
        "Puede probar con dividir a nivel de caracteres, palabras o incluso párrafos, teniendo en cuenta que el largo de cada _chunk_ no debería exceder demasiado los 500 caracteres.\n",
        "\n",
        "**Sugerencias:**\n",
        "* Puede utilizar los splitters disponibles en LangChain ([documentación](https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/)) como RecursiveCharacterTextSplitter, aunque no es obligatorio y también es correcto hacer una implementación propia.\n",
        "* Tener en cuenta que esta etapa es crucial en el resultado final. Cuanto más contextualizados queden los *chunks*, mejor será el rendimiento de la etapa de recuperación de información. Es conveniente minimizar la división de palabras (o párrafos) por la mitad."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LE5ZkBLRJQH",
        "outputId": "f2344b4f-2844-424c-e252-a7fe84451959"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Successfully loaded content from 'birds.md' ---\n"
          ]
        }
      ],
      "source": [
        "# DOCUMENTO AUXILIAR PARA PRUEBAS\n",
        "file_path = 'birds.md'\n",
        "\n",
        "try:\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        birds_document = f.read()\n",
        "    print(f\"--- Successfully loaded content from '{file_path}' ---\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at '{file_path}'\")\n",
        "    print(\"Please double-check the filename. It must be an exact match.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQ9VJF4oitZU"
      },
      "source": [
        "#### Definición de splitters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMDHuSYjiTG3",
        "outputId": "5b4b3ae2-852d-43ae-bcbe-19ea13f347b4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
        "\n",
        "from enum import StrEnum\n",
        "\n",
        "class SplitterType(StrEnum):\n",
        "    RECURSIVE = \"Recursive\"\n",
        "    MARKDOWN = \"Markdown\"\n",
        "\n",
        "## Recursive Splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 450,\n",
        "    chunk_overlap  = 40,\n",
        "    length_function = len,\n",
        "    is_separator_regex = False,\n",
        ")\n",
        "\n",
        "def splitRecursivo(text):\n",
        "  chunks = text_splitter.split_text(text)\n",
        "  return chunks\n",
        "\n",
        "## Markdown Splitter\n",
        "headers_to_split_on = [\n",
        "    (\"#\", \"Header 1\"),\n",
        "    (\"##\", \"Header 2\"),\n",
        "    (\"###\", \"Header 3\"),\n",
        "    (\"###\", \"Header 4\"),\n",
        "]\n",
        "\n",
        "markdown_splitter = MarkdownHeaderTextSplitter(\n",
        "    headers_to_split_on,\n",
        "    return_each_line=True\n",
        ")\n",
        "\n",
        "def splitMarkdown(text):\n",
        "  markdown_chunks = markdown_splitter.split_text(text)\n",
        "  chunks = text_splitter.split_documents(markdown_chunks)\n",
        "  return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PgtMbHjVLEj"
      },
      "outputs": [],
      "source": [
        "def chunk_text(text, splitter=SplitterType.RECURSIVE):\n",
        "\n",
        "  if splitter == SplitterType.RECURSIVE:\n",
        "    chunks = splitRecursivo(text)\n",
        "  elif splitter == SplitterType.MARKDOWN:\n",
        "    chunks = splitMarkdown(text)\n",
        "\n",
        "  return chunks # Lista de strings con los chunks del texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dM7jlMxFotEm"
      },
      "source": [
        "### Codigos para prueba y observar resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCvEpguOjrGs",
        "outputId": "d49f11bd-5af3-49e6-aad3-8692c4acf88b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Original Document Length: 25998 characters ---\n",
            "--- Total Chunks Generated: 93 ---\n",
            "\n",
            "--- Here are the chunks: ---\n",
            "--- CHUNK 1 (Length: 48) ---\n",
            "# **A Comprehensive Look at the World of Birds**\n",
            "--------------------\n",
            "--- CHUNK 2 (Length: 412) ---\n",
            "From the iridescent flash of a hummingbird's throat to the imperious stare of a bald eagle, birds represent a vibrant and extraordinarily successful branch of life. They have conquered every continent, from the frozen ice shelves of Antarctica to the densest tropical rainforests and the most arid deserts. With over 10,000 known species, they are the most diverse class of four-limbed vertebrates on the planet.\n",
            "--------------------\n",
            "--- CHUNK 3 (Length: 448) ---\n",
            "What defines a bird? Biologically, they are a group of endothermic (warm-blooded) vertebrates characterized by feathers, toothless beaked jaws, the laying of hard-shelled eggs, a high metabolic rate, a four-chambered heart, and a strong yet lightweight skeleton. Their ability to fly, shared by most but not all species, has allowed them to exploit ecological niches unavailable to other animals, driving their incredible diversification. This text\n",
            "--------------------\n",
            "--- CHUNK 4 (Length: 229) ---\n",
            "incredible diversification. This text will explore the world of these remarkable creatures: their ancient origins, their intricate anatomy, their complex behaviors, and their profound relationship with our planet and our species.\n",
            "--------------------\n"
          ]
        }
      ],
      "source": [
        "## Recursivo\n",
        "\n",
        "chunks = chunk_text(birds_document,SplitterType.RECURSIVE)\n",
        "\n",
        "# --- 6. View the Results ---\n",
        "print(f\"\\n--- Original Document Length: {len(birds_document)} characters ---\")\n",
        "print(f\"--- Total Chunks Generated: {len(chunks)} ---\")\n",
        "\n",
        "print(\"\\n--- Here are the chunks: ---\")\n",
        "for i, chunk in enumerate(chunks[:4]):\n",
        "    print(f\"--- CHUNK {i+1} (Length: {len(chunk)}) ---\")\n",
        "    print(chunk)\n",
        "    print(\"-\" * 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHGgvnyRY97K",
        "outputId": "6a75864d-55ad-4790-d39a-817ca34bce29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Original Document Length: 25998 characters ---\n",
            "--- Total Chunks Generated: 86 ---\n",
            "\n",
            "--- Here are the chunks: ---\n",
            "--- CHUNK 1 (Length: 412) ---\n",
            "page_content='From the iridescent flash of a hummingbird's throat to the imperious stare of a bald eagle, birds represent a vibrant and extraordinarily successful branch of life. They have conquered every continent, from the frozen ice shelves of Antarctica to the densest tropical rainforests and the most arid deserts. With over 10,000 known species, they are the most diverse class of four-limbed vertebrates on the planet.' metadata={'Header 1': '**A Comprehensive Look at the World of Birds**'}\n",
            "--------------------\n",
            "--- CHUNK 2 (Length: 448) ---\n",
            "page_content='What defines a bird? Biologically, they are a group of endothermic (warm-blooded) vertebrates characterized by feathers, toothless beaked jaws, the laying of hard-shelled eggs, a high metabolic rate, a four-chambered heart, and a strong yet lightweight skeleton. Their ability to fly, shared by most but not all species, has allowed them to exploit ecological niches unavailable to other animals, driving their incredible diversification. This text' metadata={'Header 1': '**A Comprehensive Look at the World of Birds**'}\n",
            "--------------------\n",
            "--- CHUNK 3 (Length: 229) ---\n",
            "page_content='incredible diversification. This text will explore the world of these remarkable creatures: their ancient origins, their intricate anatomy, their complex behaviors, and their profound relationship with our planet and our species.' metadata={'Header 1': '**A Comprehensive Look at the World of Birds**'}\n",
            "--------------------\n",
            "--- CHUNK 4 (Length: 422) ---\n",
            "page_content='The story of birds begins not in the sky, but on the ground, among the bipedal, meat-eating dinosaurs of the Jurassic period. For a long time, the link was controversial, but the fossil evidence is now overwhelming. Birds are not just *related* to dinosaurs; they *are* dinosaurs. Specifically, they are the last surviving lineage of theropod dinosaurs, the same group that included *Velociraptor* and *Tyrannosaurus rex*.' metadata={'Header 1': '**A Comprehensive Look at the World of Birds**', 'Header 2': '**The Avian Lineage: From Dinosaurs to Modern Birds**'}\n",
            "--------------------\n"
          ]
        }
      ],
      "source": [
        "## Markdown\n",
        "\n",
        "chunks = chunk_text(birds_document,SplitterType.MARKDOWN)\n",
        "\n",
        "# --- 6. View the Results ---\n",
        "print(f\"\\n--- Original Document Length: {len(birds_document)} characters ---\")\n",
        "print(f\"--- Total Chunks Generated: {len(chunks)} ---\")\n",
        "\n",
        "print(\"\\n--- Here are the chunks: ---\")\n",
        "for i, chunk in enumerate(chunks[:4]):\n",
        "    print(f\"--- CHUNK {i+1} (Length: {len(chunk.page_content)}) ---\")\n",
        "    print(chunk)\n",
        "    print(\"-\" * 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUVcX8t5udny",
        "outputId": "4bec6abc-3be1-4969-ec2e-d0801dfd657e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Generated 86 strings in chunks_list ---\n",
            "\n",
            "--- Example: First string ---\n",
            "**A Comprehensive Look at the World of Birds**\n",
            "From the iridescent flash of a hummingbird's throat to the imperious stare of a bald eagle, birds represent a vibrant and extraordinarily successful branch of life. They have conquered every continent, from the frozen ice shelves of Antarctica to the densest tropical rainforests and the most arid deserts. With over 10,000 known species, they are the most diverse class of four-limbed vertebrates on the planet.\n",
            "\n",
            "--- Example: Fourth string (with two headers) ---\n",
            "**A Comprehensive Look at the World of Birds**\n",
            "**The Avian Lineage: From Dinosaurs to Modern Birds**\n",
            "The story of birds begins not in the sky, but on the ground, among the bipedal, meat-eating dinosaurs of the Jurassic period. For a long time, the link was controversial, but the fossil evidence is now overwhelming. Birds are not just *related* to dinosaurs; they *are* dinosaurs. Specifically, they are the last surviving lineage of theropod dinosaurs, the same group that included *Velociraptor* and *Tyrannosaurus rex*.\n"
          ]
        }
      ],
      "source": [
        "## Si se quiere incorporar los títulos al texto\n",
        "\n",
        "def document_to_list(chunks):\n",
        "  chunks_list = []\n",
        "\n",
        "  for chunk in chunks:\n",
        "      # --- 1. Get and sort headers ---\n",
        "      header_keys = [k for k in chunk.metadata.keys() if k.startswith('Header ')]\n",
        "\n",
        "      # Sort the keys numerically, not alphabetically\n",
        "      # 'key=lambda k: int(k.split(' ')[1])' turns 'Header 10' into 10\n",
        "      header_keys.sort(key=lambda k: int(k.split(' ')[1]))\n",
        "\n",
        "      # --- 2. Build the header string ---\n",
        "\n",
        "      # Get the actual header text for each key\n",
        "      header_values = [chunk.metadata[k] for k in header_keys]\n",
        "\n",
        "      # Join them with newlines\n",
        "      header_string = \"\\n\".join(header_values)\n",
        "\n",
        "      # --- 3. Concatenate and append ---\n",
        "\n",
        "      # Add a newline between the headers and the page content\n",
        "      final_string = f\"{header_string}\\n{chunk.page_content}\"\n",
        "\n",
        "      chunks_list.append(final_string)\n",
        "\n",
        "  return chunks_list\n",
        "\n",
        "chunks_list = document_to_list(chunks)\n",
        "\n",
        "print(f\"--- Generated {len(chunks_list)} strings in chunks_list ---\")\n",
        "\n",
        "print(\"\\n--- Example: First string ---\")\n",
        "print(chunks_list[0])\n",
        "\n",
        "print(\"\\n--- Example: Fourth string (with two headers) ---\")\n",
        "print(chunks_list[3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jz42mD-ytmby"
      },
      "source": [
        "#### Métricas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPWBp10Kbdso"
      },
      "source": [
        "Ajustar a los chunks de los textos reales que usemos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jW2ZErc9fFur",
        "outputId": "6a9bc039-a38d-4c49-ceb4-4ff772d7fd8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average chunk length: 294.42 characters\n",
            "Median chunk length: 296 characters\n"
          ]
        }
      ],
      "source": [
        "all_lengths = [len(chunk.page_content) for chunk in chunks]\n",
        "\n",
        "ordered_lengths = sorted(all_lengths)\n",
        "\n",
        "average_length = sum(all_lengths) / len(all_lengths)\n",
        "\n",
        "print(f\"Average chunk length: {average_length:.2f} characters\")\n",
        "print(f\"Median chunk length: {ordered_lengths[42]} characters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7h_kw07sZwp"
      },
      "source": [
        "## Experimentando alternativas\n",
        "\n",
        "Esta parte tiene ideas que podrian ser utiles para la recuperación. Por ahora es solo copy-paste de código portencialmente útil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtSklh9wf4br",
        "outputId": "8c8aefcf-2d66-447e-928a-d5dda748ca81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Nested Header Index ---\n",
            "{'**A Comprehensive Look at the World of Birds**': {'**A Shared World: Birds and Humans**': {'**Symbols, Messengers, and Myths**': {},\n",
            "                                                                                             '**The Joy of Birding**': {},\n",
            "                                                                                             '**Threats and Conservation**': {}},\n",
            "                                                    '**A World of Variety: A Tour of Major Bird Orders**': {},\n",
            "                                                    '**From Egg to Adult: Reproduction and Life Cycle**': {'**Altricial vs. Precocial: Raising the Young**': {},\n",
            "                                                                                                           '**Building a Home: Nests and Incubation**': {},\n",
            "                                                                                                           '**Finding a Mate: Courtship Rituals**': {}},\n",
            "                                                    '**Life on the Wing: Behavior and Ecology**': {'**Flocks, Families, and Solitude: Social Structures**': {},\n",
            "                                                                                                   '**Songs, Calls, and Displays: Avian Communication**': {},\n",
            "                                                                                                   '**The Form of the Beak: Diet and Foraging**': {},\n",
            "                                                                                                   '**The Great Journeys: Migration**': {}},\n",
            "                                                    '**The Avian Lineage: From Dinosaurs to Modern Birds**': {},\n",
            "                                                    '**The Enduring Wonder of Birds**': {},\n",
            "                                                    '**The Marvel of Avian Engineering: Anatomy and Physiology**': {'**A High-Performance Engine: Metabolism and Respiration**': {},\n",
            "                                                                                                                    '**Avian Senses: A Different View of the World**': {},\n",
            "                                                                                                                    '**Feathers: The Defining Trait**': {},\n",
            "                                                                                                                    '**The Mechanics of Flight**': {}}}}\n"
          ]
        }
      ],
      "source": [
        "import pprint\n",
        "\n",
        "def build_header_index(chunks):\n",
        "    \"\"\"Builds a nested dictionary index from chunk metadata.\"\"\"\n",
        "    header_index = {}\n",
        "\n",
        "    for chunk in chunks:\n",
        "        # Get all header keys and sort them numerically\n",
        "        # (e.g., 'Header 1', 'Header 2', 'Header 10')\n",
        "        header_keys = sorted(\n",
        "            [k for k in chunk.metadata.keys() if k.startswith('Header ')],\n",
        "            key=lambda k: int(k.split(' ')[1])\n",
        "        )\n",
        "\n",
        "        # Get the actual header text values\n",
        "        header_path = [chunk.metadata[k] for k in header_keys]\n",
        "\n",
        "        # --- Build the nested dictionary ---\n",
        "        current_level = header_index\n",
        "        for header in header_path:\n",
        "            if header not in current_level:\n",
        "                current_level[header] = {}  # Create a new branch\n",
        "            current_level = current_level[header] # Move down the tree\n",
        "\n",
        "    return header_index\n",
        "\n",
        "# --- Run the function and print the result ---\n",
        "index = build_header_index(chunks)\n",
        "\n",
        "print(\"--- Nested Header Index ---\")\n",
        "pprint.pprint(index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsmvsojztGDZ"
      },
      "outputs": [],
      "source": [
        "def get_chunks_by_path(chunks, header_path):\n",
        "    \"\"\"\n",
        "    Finds all chunks that match a specific header path.\n",
        "\n",
        "    A chunk matches if its metadata path starts with the provided header_path.\n",
        "    \"\"\"\n",
        "    matching_chunks = []\n",
        "    len_path = len(header_path)\n",
        "\n",
        "    for chunk in chunks:\n",
        "        # Get all header keys and sort them numerically\n",
        "        header_keys = sorted(\n",
        "            [k for k in chunk.metadata.keys() if k.startswith('Header ')],\n",
        "            key=lambda k: int(k.split(' ')[1])\n",
        "        )\n",
        "\n",
        "        # Get the chunk's full header path\n",
        "        chunk_path = [chunk.metadata[k] for k in header_keys]\n",
        "\n",
        "        # Check if the chunk's path starts with the user's path\n",
        "        if chunk_path[:len_path] == header_path:\n",
        "            matching_chunks.append(chunk)\n",
        "\n",
        "    return matching_chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Nyqa9i7tKSq",
        "outputId": "f4b34e9a-b265-4265-ef5f-ae3ac30656d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Found 8 chunks for path: ['**A Comprehensive Look at the World of Birds**', '**The Avian Lineage: From Dinosaurs to Modern Birds**'] ---\n",
            "Chunk 1 Content: The story of birds begins not in the sky, but on the ground, among the bipedal, meat-eating dinosaurs of the Jurassic period. For a long time, the link was controversial, but the fossil evidence is now overwhelming. Birds are not just *related* to dinosaurs; they *are* dinosaurs. Specifically, they are the last surviving lineage of theropod dinosaurs, the same group that included *Velociraptor* and *Tyrannosaurus rex*.\n",
            "Chunk 1 Metadata: {'Header 1': '**A Comprehensive Look at the World of Birds**', 'Header 2': '**The Avian Lineage: From Dinosaurs to Modern Birds**'}\n",
            "\n",
            "Chunk 2 Content: The most famous transitional fossil is *Archaeopteryx*, a crow-sized creature that lived about 150 million years ago. Discovered in Germany, its fossils show a mosaic of features. Like a reptile, it had a full set of teeth, a flat breastbone, a long bony tail, and claws on its \"fingers.\" But like a bird, it was covered in fully formed, complex feathers, including asymmetrical flight feathers on its wings—a clear indication it was capable of at\n",
            "Chunk 2 Metadata: {'Header 1': '**A Comprehensive Look at the World of Birds**', 'Header 2': '**The Avian Lineage: From Dinosaurs to Modern Birds**'}\n",
            "\n",
            "Chunk 3 Content: clear indication it was capable of at least some form of flight, likely gliding or weak flapping.\n",
            "Chunk 3 Metadata: {'Header 1': '**A Comprehensive Look at the World of Birds**', 'Header 2': '**The Avian Lineage: From Dinosaurs to Modern Birds**'}\n",
            "\n",
            "Chunk 4 Content: *Archaeopteryx* was long considered the \"first bird,\" but recent discoveries, particularly from the rich fossil beds of China, have painted a more complex picture. We now know of many \"dino-birds\" and \"bird-like dinosaurs\" that were covered in a wide array of feathers, from simple filaments to complex plumage. It seems feathers evolved long before flight, likely first for insulation or display. Dinosaurs like *Velociraptor* almost certainly had\n",
            "Chunk 4 Metadata: {'Header 1': '**A Comprehensive Look at the World of Birds**', 'Header 2': '**The Avian Lineage: From Dinosaurs to Modern Birds**'}\n",
            "\n",
            "Chunk 5 Content: *Velociraptor* almost certainly had feathers.\n",
            "Chunk 5 Metadata: {'Header 1': '**A Comprehensive Look at the World of Birds**', 'Header 2': '**The Avian Lineage: From Dinosaurs to Modern Birds**'}\n",
            "\n",
            "Chunk 6 Content: This burst of evolution meant that when the K-Pg extinction event occurred 66 million years ago—the asteroid impact that wiped out the non-avian dinosaurs, pterosaurs, and large marine reptiles—a diverse group of early birds was already present. Why did they survive when their larger relatives perished? The exact reasons are debated, but their small size, ability to fly (allowing them to escape local devastation), and perhaps a diet of seeds (a\n",
            "Chunk 6 Metadata: {'Header 1': '**A Comprehensive Look at the World of Birds**', 'Header 2': '**The Avian Lineage: From Dinosaurs to Modern Birds**'}\n",
            "\n",
            "Chunk 7 Content: and perhaps a diet of seeds (a food source that would have been plentiful after the post-impact winter) are all leading theories.\n",
            "Chunk 7 Metadata: {'Header 1': '**A Comprehensive Look at the World of Birds**', 'Header 2': '**The Avian Lineage: From Dinosaurs to Modern Birds**'}\n",
            "\n",
            "Chunk 8 Content: The handful of bird lineages that survived this apocalypse found themselves in a new, empty world. With their main predators and competitors gone, they underwent an explosive radiation in the Cenozoic Era. This is when the ancestors of all modern bird groups—ducks, owls, penguins, songbirds, and all the rest—emerged and diversified, evolving to fill the myriad ecological roles we see them in today.\n",
            "Chunk 8 Metadata: {'Header 1': '**A Comprehensive Look at the World of Birds**', 'Header 2': '**The Avian Lineage: From Dinosaurs to Modern Birds**'}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "path_h2 = [\n",
        "    '**A Comprehensive Look at the World of Birds**',\n",
        "    '**The Avian Lineage: From Dinosaurs to Modern Birds**'\n",
        "]\n",
        "\n",
        "results_h2 = get_chunks_by_path(chunks, path_h2)\n",
        "\n",
        "print(f\"\\n--- Found {len(results_h2)} chunks for path: {path_h2} ---\")\n",
        "for i, chunk in enumerate(results_h2):\n",
        "    print(f\"Chunk {i+1} Content: {chunk.page_content}\")\n",
        "    print(f\"Chunk {i+1} Metadata: {chunk.metadata}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74-bH4j_VrRT"
      },
      "outputs": [],
      "source": [
        "chunks = []\n",
        "for document in documents:\n",
        "  chunks += chunk_text(document)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJ2wgEx3N-O3"
      },
      "source": [
        "## Parte 3: Recuperación de información\n",
        "\n",
        "En esta parte vamos a implementar el método de recuperación de información que nos permitirá obtener los _chunks_ más relevantes para la pregunta.\n",
        "\n",
        "En primer lugar, cargamos el modelo Bi-Encoder que utilizaremos para generar los embeddings utilizando la librería sentence_transformers.\n",
        "\n",
        "Se utiliza el modelo multilingüe [intfloat/multilingual-e5-large](https://huggingface.co/intfloat/multilingual-e5-large), fine-tuning del modelo `xlm-roberta-large` para la tarea de generación de sentence embeddings.\n",
        "\n",
        "Se pueden explorar otros modelos Bi-Encoder, e incluso modelos Cross-Encoder o del tipo ColBERT. En HuggingFace se puede consultar el siguiente [leaderboard](https://huggingface.co/spaces/mteb/leaderboard) que compara varios modelos de este tipo en diferentes tareas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Fozn2H7vZsj"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model_emb = SentenceTransformer(\"intfloat/multilingual-e5-large\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwuNrG2djc3f"
      },
      "source": [
        "A continuación se debe generar las representaciones vectoriales para todos los _chunks_ ([ejemplo de uso](https://huggingface.co/intfloat/multilingual-e5-large#support-for-sentence-transformers)).\n",
        "\n",
        "**Observación:** El modelo que estamos usando espera que los _chunks_ comiencen con el prefijo `passage: ` por lo que será necesario agregarlo al inicio de todos los _chunks_."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WeBfJ_Ubjb3k",
        "outputId": "71037f7a-a3bc-4028-f2cf-07fd99ebb31a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generando embeddings para 1 chunks...\n",
            "Embeddings generados. Dimensión: (1, 1024)\n"
          ]
        }
      ],
      "source": [
        "def prepare_chunks_for_embedding(chunks):\n",
        "    \"\"\"\n",
        "    Prepara los chunks agregando el prefijo requerido por el modelo.\n",
        "    Maneja tanto strings como objetos Document de LangChain.\n",
        "    \"\"\"\n",
        "    prepared_chunks = []\n",
        "    for chunk in chunks:\n",
        "        # Si es un objeto Document (del markdown splitter), extraer el contenido\n",
        "        if hasattr(chunk, 'page_content'):\n",
        "            text = chunk.page_content\n",
        "        else:\n",
        "            # Si es un string simple\n",
        "            text = chunk\n",
        "\n",
        "        prepared_chunks.append(f\"passage: {text}\")\n",
        "\n",
        "    return prepared_chunks\n",
        "\n",
        "chunks_with_prefix = prepare_chunks_for_embedding(chunks)\n",
        "\n",
        "print(f\"Generando embeddings para {len(chunks_with_prefix)} chunks...\")\n",
        "chunk_embeddings = model_emb.encode(chunks_with_prefix, normalize_embeddings=True)\n",
        "\n",
        "print(f\"Embeddings generados. Dimensión: {chunk_embeddings.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N62kl88KwZ-u"
      },
      "source": [
        "Por último, se debe implementar el algoritmo de búsqueda de los embeddings más cercanos para un embedding dado.\n",
        "\n",
        "**Sugerencias:**\n",
        "* Utilizar la clase NearestNeighbors de sklearn ([documentación](https://scikit-learn.org/dev/modules/generated/sklearn.neighbors.NearestNeighbors.html#sklearn.neighbors.NearestNeighbors))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5bGq2Etxkj-",
        "outputId": "e49ee795-04aa-401e-9696-50703aef6ddf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modelo de vecinos más cercanos entrenado\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "nn_model = NearestNeighbors(\n",
        "    n_neighbors=5,  # Por ahora buscamos 5 vecinos, ajustamos despues\n",
        "    metric='cosine',\n",
        "    algorithm='brute'  # 'brute' supuestamente es más preciso para datasets chicos/medianos\n",
        ")\n",
        "\n",
        "nn_model.fit(chunk_embeddings)\n",
        "\n",
        "print(\"Modelo de vecinos más cercanos entrenado\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKTmCneGbds5"
      },
      "outputs": [],
      "source": [
        "def retrieve_chunks(query, top_k=3):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "        Una tupla (chunks_recuperados, distancias, indices)\n",
        "        - chunks_recuperados: Lista de textos de los chunks más relevantes\n",
        "        - distancias: Distancias coseno a cada chunk\n",
        "        - indices: Índices de los chunks en la lista original\n",
        "    \"\"\"\n",
        "    query_with_prefix = f\"query: {query}\"\n",
        "\n",
        "    query_embedding = model_emb.encode([query_with_prefix], normalize_embeddings=True)\n",
        "\n",
        "    distances, indices = nn_model.kneighbors(query_embedding, n_neighbors=top_k)\n",
        "\n",
        "    # Extraer los chunks correspondientes\n",
        "    retrieved_chunks = []\n",
        "    for idx in indices[0]:\n",
        "        chunk = chunks[idx]\n",
        "        # Manejar tanto strings como objetos Document\n",
        "        if hasattr(chunk, 'page_content'):\n",
        "            retrieved_chunks.append(chunk.page_content)\n",
        "        else:\n",
        "            retrieved_chunks.append(chunk)\n",
        "\n",
        "    return retrieved_chunks, distances[0], indices[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwMSqjQUOEzQ"
      },
      "source": [
        "## Parte 4: Generación de respuestas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyPn_Cwtq81r"
      },
      "source": [
        "### Configuración de LLM\n",
        "\n",
        "En esta parte, implementaremos un wrapper flexible que permite experimentar con diferentes modelos de lenguaje:\n",
        "\n",
        "1. **Llama 3.1** (modelo abierto): Utilizaremos el modelo Meta-Llama-3.1-8B-Instruct a través de HuggingFace.\n",
        "2. **Gemini 2.0 Flash** (modelo cerrado): Utilizaremos la API de Google Gemini.\n",
        "\n",
        "Para **Llama 3.1**, es necesario:\n",
        "- Crearse una cuenta de HuggingFace (https://huggingface.co/)\n",
        "- Aceptar los términos para usar el modelo en HuggingFace: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct\n",
        "- Crear un token de HuggingFace con permiso de lectura: https://huggingface.co/settings/tokens\n",
        "\n",
        "Para **Gemini**, es necesario:\n",
        "- Obtener una API key de Google AI Studio: https://aistudio.google.com/app/apikey"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpJk4QSGAPkE",
        "outputId": "8be0f88a-eb36-4bf4-eefe-16fee32ccb19"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "The `notebook_login` function can only be used in a notebook (Jupyter or Colab) and you need the `ipywidgets` module: `pip install ipywidgets`.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/huggingface_hub/_login.py:334\u001b[39m, in \u001b[36mnotebook_login\u001b[39m\u001b[34m(new_session, write_permission)\u001b[39m\n\u001b[32m    333\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mipywidgets\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mwidgets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwidgets\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    335\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m display  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'ipywidgets'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Ejecutar para conectarse a HuggingFace (solo necesario para Llama 3.1)\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhuggingface_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m notebook_login\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mnotebook_login\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:101\u001b[39m, in \u001b[36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     99\u001b[39m         message += \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + custom_message\n\u001b[32m    100\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:31\u001b[39m, in \u001b[36m_deprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     29\u001b[39m extra_args = \u001b[38;5;28mlen\u001b[39m(args) - \u001b[38;5;28mlen\u001b[39m(all_args)\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m extra_args <= \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# extra_args > 0\u001b[39;00m\n\u001b[32m     33\u001b[39m args_msg = [\n\u001b[32m     34\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m name, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(kwonly_args[:extra_args], args[-extra_args:])\n\u001b[32m     36\u001b[39m ]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/lib/python3.11/site-packages/huggingface_hub/_login.py:337\u001b[39m, in \u001b[36mnotebook_login\u001b[39m\u001b[34m(new_session, write_permission)\u001b[39m\n\u001b[32m    335\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m display  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe `notebook_login` function can only be used in a notebook (Jupyter or\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m Colab) and you need the `ipywidgets` module: `pip install ipywidgets`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m     )\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m new_session \u001b[38;5;129;01mand\u001b[39;00m get_token() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    342\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mUser is already logged in.\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mImportError\u001b[39m: The `notebook_login` function can only be used in a notebook (Jupyter or Colab) and you need the `ipywidgets` module: `pip install ipywidgets`."
          ]
        }
      ],
      "source": [
        "# Ejecutar para conectarse a HuggingFace (solo necesario para Llama 3.1)\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "if COLAB:\n",
        "  notebook_login()\n",
        "else:\n",
        "    # Loguear localmente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fPqX6DH1Gzn"
      },
      "source": [
        "### Definición del Wrapper de LLM\n",
        "\n",
        "A continuación se define una clase abstracta que permite intercambiar fácilmente entre diferentes modelos de lenguaje."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVPu5MUiTBog"
      },
      "outputs": [],
      "source": [
        "from abc import ABC, abstractmethod\n",
        "from typing import Optional\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "class LLMWrapper(ABC):\n",
        "    \"\"\"Clase base abstracta para wrappers de modelos de lenguaje.\"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def generate(self, prompt: str, temperature: float = 0.0, max_tokens: int = 500) -> str:\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_model_name(self) -> str:\n",
        "        pass\n",
        "\n",
        "\n",
        "class LlamaWrapper(LLMWrapper):\n",
        "    \"\"\"Wrapper para el modelo Llama 3.1 de Meta vía HuggingFace.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "        import torch\n",
        "\n",
        "        print(\"Inicializando Llama 3.1...\")\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            \"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
        "\n",
        "        # Configuración de cuantización a 4 bits (para mejorar eficiencia)\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16)\n",
        "\n",
        "        # Inicializar el modelo\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
        "            quantization_config=bnb_config,\n",
        "            device_map=\"auto\")\n",
        "\n",
        "        print(\"Llama 3.1 inicializado correctamente\")\n",
        "\n",
        "    def generate(self, prompt: str, temperature: float = 0.0, max_tokens: int = 500) -> str:\n",
        "        from transformers import GenerationConfig, pipeline\n",
        "\n",
        "        # Configuración de temperatura\n",
        "        generation_config = GenerationConfig(\n",
        "            temperature=temperature if temperature > 0 else None,\n",
        "            do_sample=temperature > 0)\n",
        "\n",
        "        # Inicializar pipeline para generación de texto\n",
        "        pipe = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=self.model,\n",
        "            config=generation_config,\n",
        "            tokenizer=self.tokenizer,\n",
        "            pad_token_id=self.tokenizer.eos_token_id)\n",
        "\n",
        "        # Generar texto\n",
        "        output = pipe(\n",
        "            prompt,\n",
        "            return_full_text=False,\n",
        "            max_new_tokens=max_tokens)\n",
        "\n",
        "        return output[0]['generated_text']\n",
        "\n",
        "    def get_model_name(self) -> str:\n",
        "        return \"Llama-3.1-8B-Instruct\"\n",
        "\n",
        "\n",
        "class GeminiWrapper(LLMWrapper):\n",
        "    \"\"\"Wrapper para el modelo Gemini 2.0 Flash de Google.\"\"\"\n",
        "\n",
        "    def __init__(self, api_key: Optional[str] = None):\n",
        "        from google import genai\n",
        "\n",
        "        print(\"Inicializando Gemini 2.0 Flash...\")\n",
        "\n",
        "        # Obtener API key\n",
        "        if api_key is None:\n",
        "            api_key = os.environ.get(\"GEMINI_API_KEY\")\n",
        "\n",
        "        if api_key is None:\n",
        "            print(\"No se encontró la API key de Gemini en las variables de entorno.\")\n",
        "            api_key = getpass(\"Por favor, ingrese su API key de Gemini: \")\n",
        "            # Guardar en variables de entorno para esta sesión\n",
        "            os.environ[\"GEMINI_API_KEY\"] = api_key\n",
        "\n",
        "        self.client = genai.Client(api_key=api_key)\n",
        "\n",
        "        print(\"Gemini 2.0 Flash inicializado correctamente\")\n",
        "\n",
        "    def generate(self, prompt: str, temperature: float = 0.0, max_tokens: int = 500) -> str:\n",
        "        from google.genai import types\n",
        "\n",
        "        config = types.GenerateContentConfig(\n",
        "            temperature=temperature if temperature > 0 else 0.0,\n",
        "            max_output_tokens=max_tokens)\n",
        "\n",
        "        # Generar respuesta\n",
        "        response = self.client.models.generate_content(\n",
        "            model=\"gemini-2.0-flash-exp\",\n",
        "            contents=prompt,\n",
        "            config=config)\n",
        "\n",
        "        return response.text\n",
        "\n",
        "    def get_model_name(self) -> str:\n",
        "        return \"Gemini-2.0-Flash\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chM4PPdWbds_"
      },
      "source": [
        "### Instanciar modelos\n",
        "\n",
        "Seleccionar qué modelo(s) se va a inicializar. Se puede inicializar ambos para facilitar la experimentación posterior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQqR9jI7bdtA"
      },
      "outputs": [],
      "source": [
        "# Descomentar el(los) modelo(s) que se quiera utilizar\n",
        "\n",
        "llama_model = LlamaWrapper()\n",
        "\n",
        "gemini_model = GeminiWrapper()\n",
        "\n",
        "# Seleccionar el modelo activo para los experimentos\n",
        "active_model = llama_model  # o gemini_model\n",
        "\n",
        "print(f\"\\nModelo activo: {active_model.get_model_name()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgGESsQX1k5_"
      },
      "source": [
        "### Función auxiliar para generación de respuestas\n",
        "\n",
        "Esta función utiliza el modelo activo seleccionado anteriormente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jyf3eQPB4zQz"
      },
      "outputs": [],
      "source": [
        "def get_response(\n",
        "    prompt: str,\n",
        "    model: LLMWrapper = None,\n",
        "    temp: float = 0.0,\n",
        "    max_tok: int = 500\n",
        ") -> str:\n",
        "    if model is None:\n",
        "        model = active_model\n",
        "\n",
        "    return model.generate(prompt, temperature=temp, max_tokens=max_tok)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PN9wyhSm5Mtj"
      },
      "source": [
        "### Crear prompt y generar respuesta\n",
        "\n",
        "Escribir la función `create_prompt(question, use_chat_template=True)` que dada una pregunta, genere la prompt que se utilizará para generar la respuesta. Tener en cuenta que se debe realizar la búsqueda semántica de los _chunks_ más cercanos a la pregunta utilizando lo implementado en la parte 3.\n",
        "\n",
        "**Observación:** Al igual que para los _chunks_, el modelo Bi-Encoder espera que la pregunta comience con un prefijo especial: `query: ` por lo que será necesario agregarlo al inicio de la pregunta para generar el embedding.\n",
        "\n",
        "**Sugerencias:**\n",
        "* Puede probar con distintas cantidades de _chunks_ recuperados, pero se sugiere comenzar con 3. Tener en cuenta que más _chunks_ recuperados y agregados en la prompt implica mayor uso de memoria en inferencia.\n",
        "* El parámetro `use_chat_template` permite controlar si se aplica el template de chat de Llama (necesario para Llama 3.1, opcional para Gemini). Para Llama usar `True`, para Gemini se puede probar con `True` o `False` según el formato que se prefiera."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQtVcehH-nQJ"
      },
      "outputs": [],
      "source": [
        "def create_prompt(question, use_chat_template=True, model_for_template=None, top_k=3):\n",
        "    \"\"\"\n",
        "    Crea el prompt para el modelo de lenguaje incluyendo contexto recuperado.\n",
        "\n",
        "    Args:\n",
        "        question: La pregunta del usuario\n",
        "        use_chat_template: Si True, aplica el template de chat de Llama\n",
        "        model_for_template: Modelo del cual usar el tokenizer (solo para Llama)\n",
        "        top_k: Número de chunks a recuperar (por defecto 3)\n",
        "\n",
        "    Returns:\n",
        "        El prompt formateado\n",
        "    \"\"\"\n",
        "    # 1. Recuperar chunks relevantes usando búsqueda semántica\n",
        "    # La función retrieve_chunks ya maneja el prefijo \"query:\" internamente\n",
        "    retrieved_chunks, _, _ = retrieve_chunks(question, top_k=top_k)\n",
        "\n",
        "    # 2. Construir el contexto con los chunks recuperados\n",
        "    context = \"\\n\\n\".join(retrieved_chunks)\n",
        "\n",
        "    # 3. Crear el mensaje del sistema y del usuario\n",
        "    system_message = \"\"\"Eres un asistente experto en responder preguntas basándote únicamente en el contexto proporcionado.\n",
        "\n",
        "Instrucciones:\n",
        "- Si la información para responder la pregunta está en el contexto, proporciona una respuesta clara y concisa.\n",
        "- Si la información NO está en el contexto, responde: \"Lo siento, no cuento con información para responder esa pregunta.\"\n",
        "- No inventes información que no esté en el contexto.\n",
        "- Responde en español.\"\"\"\n",
        "\n",
        "    user_message = f\"\"\"Contexto:\n",
        "{context}\n",
        "\n",
        "Pregunta: {question}\"\"\"\n",
        "\n",
        "    # 4. Aplicar chat template si corresponde\n",
        "    if use_chat_template:\n",
        "        # Usar tokenizer de Llama para aplicar template\n",
        "        if model_for_template is None and isinstance(active_model, LlamaWrapper):\n",
        "            model_for_template = active_model\n",
        "\n",
        "        if isinstance(model_for_template, LlamaWrapper):\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": system_message},\n",
        "                {\"role\": \"user\", \"content\": user_message}\n",
        "            ]\n",
        "            prompt = model_for_template.tokenizer.apply_chat_template(\n",
        "                messages, tokenize=False, add_generation_prompt=True\n",
        "            )\n",
        "        else:\n",
        "            # Para Gemini u otros modelos sin template específico\n",
        "            prompt = f\"{system_message}\\n\\n{user_message}\"\n",
        "    else:\n",
        "        # Sin template, formato simple\n",
        "        prompt = f\"{system_message}\\n\\n{user_message}\"\n",
        "\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfYv8FZ2-0fR"
      },
      "source": [
        "Probar la prompt anterior con un ejemplo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqESfBkq-wF6"
      },
      "outputs": [],
      "source": [
        "question = \"\"  # Completar con una pregunta adecuada al contexto\n",
        "\n",
        "# Crear prompt según el modelo activo\n",
        "use_template = isinstance(active_model, LlamaWrapper)  # True para Llama, False para Gemini\n",
        "prompt = create_prompt(question, use_chat_template=use_template)\n",
        "\n",
        "print(f\"MODELO: {active_model.get_model_name()}\")\n",
        "print(\"\\nPROMPT:\")\n",
        "print(prompt)\n",
        "print(\"\\nRESPUESTA:\")\n",
        "print(get_response(prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYOvIEHAOJSc"
      },
      "source": [
        "## Parte 5: Evaluación\n",
        "A continuación vamos a evaluar la solución construida. Para ello, se deben seguir los siguientes pasos:\n",
        "\n",
        "* Construir un conjunto de evaluación de forma manual que contenga al menos 12 preguntas y respuestas con las siguientes características:\n",
        "  * Al menos 3 preguntas deben necesitar información presente en más de un _chunk_ para ser respondidas correctamente.\n",
        "  * Al menos 3 preguntas no deben estar relacionadas con el dominio, y su respuesta de referencia debe ser algo similar a: \"Lo siento, no cuento con información para responder esa pregunta.\"\n",
        "* El conjunto debe estar en un archivo CSV llamado testset.csv, con las columnas \"question\" y \"answer\".\n",
        "\n",
        "Se deberá realizar al menos tres experimentos diferentes y evaluar sobre el mismo conjunto de test con la métrica BERTScore. Los experimentos deben variar en al menos uno de los siguientes elementos:\n",
        "* Método de chunking\n",
        "* Modelo (o método) de retrieval\n",
        "* Modelo de generación (LLM)\n",
        "* Método de prompting (se puede probar con few-shot, chain of thought, etc)\n",
        "* Otros aspectos que considere relevantes a probar\n",
        "\n",
        "A continuación se definen funciones auxiliares para la evaluación.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkFBE_iEKkkm"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def generate_predictions(questions, model=None, use_chat_template=True):\n",
        "    if model is None:\n",
        "        model = active_model\n",
        "\n",
        "    predictions = []\n",
        "    for question in tqdm(questions, desc=f\"Generando con {model.get_model_name()}\"):\n",
        "        prompt = create_prompt(question, use_chat_template=use_chat_template)\n",
        "        prediction = get_response(prompt, model=model)\n",
        "        predictions.append(prediction)\n",
        "\n",
        "    return predictions\n",
        "\n",
        "def evaluate_predictions(predictions, references, experiment_name=\"\"):\n",
        "    \"\"\"Evalúa predicciones usando BERTScore.\"\"\"\n",
        "    bertscore = evaluate.load(\"bertscore\")\n",
        "    results = bertscore.compute(predictions=predictions, references=references, lang='es')\n",
        "\n",
        "    metrics = {\n",
        "        'precision': np.array(results['precision']).mean(),\n",
        "        'recall': np.array(results['recall']).mean(),\n",
        "        'f1': np.array(results['f1']).mean()\n",
        "    }\n",
        "\n",
        "    if experiment_name:\n",
        "        print(f\"\\n=== Resultados: {experiment_name} ===\")\n",
        "    print(f\"BERTScore P:  {metrics['precision']:.3f}\")\n",
        "    print(f\"BERTScore R:  {metrics['recall']:.3f}\")\n",
        "    print(f\"BERTScore F1: {metrics['f1']:.3f}\")\n",
        "\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-wIITRgKKoE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Leer el conjunto de evaluación\n",
        "df = pd.read_csv(\"testset.csv\")\n",
        "\n",
        "# Obtener preguntas y respuestas\n",
        "questions = df[\"question\"].tolist()\n",
        "references = df[\"answer\"].tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8quL9-CVKEB"
      },
      "source": [
        "Evalúe los experimentos realizados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUnRnoksVOv-"
      },
      "outputs": [],
      "source": [
        "# Evaluar experimentos\n",
        "# Almacenar resultados para comparación posterior\n",
        "results_dict = {}\n",
        "\n",
        "# ============================================================================\n",
        "# EXPERIMENTO 1: Llama 3.1 con chat template\n",
        "# ============================================================================\n",
        "exp1_model = llama_model\n",
        "exp1_name = \"Exp1: Llama 3.1 con chat template\"\n",
        "\n",
        "predictions_exp1 = generate_predictions(questions, model=exp1_model, use_chat_template=True)\n",
        "results_dict[exp1_name] = evaluate_predictions(predictions_exp1, references, exp1_name)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# EXPERIMENTO 2: Gemini 2.0 Flash\n",
        "# ============================================================================\n",
        "# exp2_model = gemini_model\n",
        "# exp2_name = \"Exp2: Gemini 2.0 Flash\"\n",
        "#\n",
        "# predictions_exp2 = generate_predictions(questions, model=exp2_model, use_chat_template=False)\n",
        "# results_dict[exp2_name] = evaluate_predictions(predictions_exp2, references, exp2_name)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# EXPERIMENTO 3: Otro experimento (acá variamos lo que dijimos)\n",
        "# ============================================================================\n",
        "# exp3_model = active_model  # Usar el modelo que ande mejor, por ej.\n",
        "# exp3_name = \"Exp3: [Descripción del experimento]\"\n",
        "#\n",
        "# # Ejemplo: se puede modificar create_prompt para usar diferente estrategia\n",
        "# predictions_exp3 = generate_predictions(questions, model=exp3_model, use_chat_template=True)\n",
        "# results_dict[exp3_name] = evaluate_predictions(predictions_exp3, references, exp3_name)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# RESUMEN DE RESULTADOS\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"RESUMEN DE TODOS LOS EXPERIMENTOS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "import pandas as pd\n",
        "summary_df = pd.DataFrame(results_dict).T\n",
        "summary_df.columns = ['Precision', 'Recall', 'F1']\n",
        "print(summary_df.to_string())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1GKFIKIVk42"
      },
      "source": [
        "### Reporte de resultados\n",
        "\n",
        "Reportar los resultados obtenidos en los experimentos realizados completando la siguiente tabla:\n",
        "\n",
        "| Exp | Descripción | P BERTScore | R BERTScore | F BERTScore |\n",
        "|-----|-------------|-------------|-------------|-------------|\n",
        "| 1 | | | | |\n",
        "| 2 | | | | |\n",
        "| 3 | | | | |\n",
        "\n",
        "Responda las siguientes preguntas:\n",
        "\n",
        "1. Explique brevemente las diferencias en los experimentos realizados, ¿Qué aspectos se varió en el pipeline de RAG?\n",
        "\n",
        "2. ¿Son consistentes los resultados obtenidos con lo que esperaba?\n",
        "\n",
        "3. ¿Le parece que la métrica BERTScore está capturando correctamente las diferencias de los distintos experimentos realizados?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peHQat3XWzX2"
      },
      "source": [
        "(sus respuestas aquí)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "FtgdalArNvHn",
        "gCpCYmSrN6oK",
        "XJ2wgEx3N-O3",
        "qwMSqjQUOEzQ",
        "LYOvIEHAOJSc"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}